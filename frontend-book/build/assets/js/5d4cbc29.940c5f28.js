"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[605],{7966(n,e,i){i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>s,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"module-2-digital-twin/chapter-5","title":"Chapter 5: Simulation to Real-World Transfer","description":"Techniques and challenges for transferring AI models from simulation to real robots","source":"@site/docs/module-2-digital-twin/chapter-5.md","sourceDirName":"module-2-digital-twin","slug":"/module-2-digital-twin/chapter-5","permalink":"/HACKATHONN-1-Physical-AI-And-Humanoids-Robotics-Textbook/docs/module-2-digital-twin/chapter-5","draft":false,"unlisted":false,"editUrl":"https://github.com/muhammadyasir678/HACKATHONN-1-Physical-AI-And-Humanoids-Robotics-Textbook/tree/main/docs/module-2-digital-twin/chapter-5.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"Chapter 5: Simulation to Real-World Transfer","sidebar_position":5,"description":"Techniques and challenges for transferring AI models from simulation to real robots"},"sidebar":"textbookSidebar","previous":{"title":"Chapter 4: Unity Integration for Advanced Simulation","permalink":"/HACKATHONN-1-Physical-AI-And-Humanoids-Robotics-Textbook/docs/module-2-digital-twin/chapter-4"},"next":{"title":"Chapter 1: Isaac Sim Overview and Setup","permalink":"/HACKATHONN-1-Physical-AI-And-Humanoids-Robotics-Textbook/docs/module-3-ai-robot-brain/chapter-1"}}');var a=i(4848),r=i(8453);const s={title:"Chapter 5: Simulation to Real-World Transfer",sidebar_position:5,description:"Techniques and challenges for transferring AI models from simulation to real robots"},o="Chapter 5: Simulation to Real-World Transfer",l={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Core Theory",id:"core-theory",level:2},{value:"Practical Example",id:"practical-example",level:2},{value:"Code Snippet",id:"code-snippet",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Summary",id:"summary",level:2}];function m(n){const e={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"chapter-5-simulation-to-real-world-transfer",children:"Chapter 5: Simulation to Real-World Transfer"})}),"\n",(0,a.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Understand the concept of sim-to-real transfer in robotics"}),"\n",(0,a.jsx)(e.li,{children:"Learn about domain randomization and domain adaptation techniques"}),"\n",(0,a.jsx)(e.li,{children:"Explore methods to bridge the reality gap between simulation and real robots"}),"\n",(0,a.jsx)(e.li,{children:"Gain knowledge of evaluation and validation approaches for transfer learning"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(e.p,{children:'Simulation-to-real-world transfer (sim-to-real) is a critical challenge in Physical AI and robotics. While simulation provides a safe, cost-effective, and controllable environment for training AI models, the ultimate goal is to deploy these models on real robots. The "reality gap" between simulated and real environments often prevents directly transferring models from simulation to reality. Understanding and addressing this gap is essential for practical Physical AI applications.'}),"\n",(0,a.jsx)(e.h2,{id:"core-theory",children:"Core Theory"}),"\n",(0,a.jsx)(e.p,{children:"The reality gap encompasses several factors:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Visual Differences"}),": Lighting, textures, colors, and visual artifacts"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Physical Differences"}),": Dynamics, friction, material properties, and sensor noise"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Modeling Imperfections"}),": Simplified physics, inaccurate robot models, and environmental assumptions"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Temporal Differences"}),": Timing, latency, and update rates"]}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:"Techniques to address the reality gap include:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Domain Randomization"}),": Training with randomized simulation parameters"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Domain Adaptation"}),": Adapting models to new domains using limited real data"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"System Identification"}),": Calibrating simulation parameters to match reality"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Progressive Domain Transfer"}),": Gradual transition from simulation to reality"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Sim-to-Real Systematic Approach"}),": Methodical validation and adjustment"]}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:"Domain randomization involves varying parameters during training such as:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Lighting conditions and camera properties"}),"\n",(0,a.jsx)(e.li,{children:"Object textures, colors, and appearances"}),"\n",(0,a.jsx)(e.li,{children:"Physical properties (friction, mass, damping)"}),"\n",(0,a.jsx)(e.li,{children:"Sensor noise characteristics"}),"\n",(0,a.jsx)(e.li,{children:"Environmental conditions"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"practical-example",children:"Practical Example"}),"\n",(0,a.jsx)(e.p,{children:"Let's examine domain randomization implementation in simulation:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nimport random\nimport numpy as np\nfrom std_msgs.msg import Float32\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\n\nclass DomainRandomizationNode(Node):\n    def __init__(self):\n        super().__init__('domain_randomization_node')\n        self.bridge = CvBridge()\n\n        # Publishers for randomized parameters\n        self.friction_pub = self.create_publisher(Float32, '/randomized_friction', 10)\n        self.lighting_pub = self.create_publisher(Float32, '/randomized_lighting', 10)\n\n        # Timer for parameter updates\n        self.randomization_timer = self.create_timer(5.0, self.randomize_parameters)\n\n        # Store current parameters\n        self.current_friction = 0.5\n        self.current_lighting = 1.0\n\n        self.get_logger().info('Domain randomization node initialized')\n\n    def randomize_parameters(self):\n        \"\"\"Randomize simulation parameters\"\"\"\n        # Randomize friction coefficient (0.1 to 1.0)\n        self.current_friction = random.uniform(0.1, 1.0)\n        friction_msg = Float32()\n        friction_msg.data = self.current_friction\n        self.friction_pub.publish(friction_msg)\n\n        # Randomize lighting intensity (0.5 to 2.0)\n        self.current_lighting = random.uniform(0.5, 2.0)\n        lighting_msg = Float32()\n        lighting_msg.data = self.current_lighting\n        self.lighting_pub.publish(lighting_msg)\n\n        self.get_logger().info(\n            f'Randomized parameters - Friction: {self.current_friction:.2f}, '\n            f'Lighting: {self.current_lighting:.2f}'\n        )\n\n    def randomize_visual_features(self, image):\n        \"\"\"Apply visual domain randomization to an image\"\"\"\n        # Convert ROS image to OpenCV\n        cv_image = self.bridge.imgmsg_to_cv2(image, desired_encoding='bgr8')\n\n        # Apply random color jittering\n        brightness_factor = random.uniform(0.7, 1.3)\n        contrast_factor = random.uniform(0.8, 1.2)\n        saturation_factor = random.uniform(0.9, 1.1)\n\n        # Convert to HSV for easier manipulation\n        hsv = cv2.cvtColor(cv_image, cv2.COLOR_BGR2HSV).astype(np.float32)\n        hsv[:, :, 2] = hsv[:, :, 2] * brightness_factor  # V channel\n        hsv = np.clip(hsv, 0, 255).astype(np.uint8)\n\n        # Convert back to BGR\n        randomized_image = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n\n        # Add random noise\n        noise = np.random.normal(0, random.uniform(1, 5), randomized_image.shape).astype(np.uint8)\n        randomized_image = cv2.add(randomized_image, noise)\n\n        return randomized_image\n\ndef main(args=None):\n    rclpy.init(args=args)\n    domain_randomization_node = DomainRandomizationNode()\n\n    try:\n        rclpy.spin(domain_randomization_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        domain_randomization_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"code-snippet",children:"Code Snippet"}),"\n",(0,a.jsx)(e.p,{children:"Example of system identification for sim-to-real transfer:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom std_msgs.msg import Float32MultiArray\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import JointState\nimport pickle\n\nclass SystemIdentificationNode(Node):\n    def __init__(self):\n        super().__init__('system_identification_node')\n\n        # Subscribers for real robot data\n        self.joint_state_sub = self.create_subscription(\n            JointState,\n            '/joint_states',\n            self.joint_state_callback,\n            10\n        )\n\n        self.cmd_sub = self.create_subscription(\n            Twist,\n            '/cmd_vel',\n            self.cmd_callback,\n            10\n        )\n\n        # Publisher for identified parameters\n        self.param_pub = self.create_publisher(Float32MultiArray, '/identified_params', 10)\n\n        # Data storage for system identification\n        self.joint_positions = []\n        self.joint_velocities = []\n        self.commands = []\n        self.timestamps = []\n\n        # Timer for parameter identification\n        self.identification_timer = self.create_timer(10.0, self.perform_identification)\n\n        # Initial parameter estimates\n        self.current_params = np.array([1.0, 0.1, 0.05])  # mass, damping, friction\n\n        self.get_logger().info('System identification node initialized')\n\n    def joint_state_callback(self, msg):\n        \"\"\"Store joint state data for system identification\"\"\"\n        self.joint_positions.append(list(msg.position))\n        self.joint_velocities.append(list(msg.velocity))\n        self.timestamps.append(self.get_clock().now().nanoseconds / 1e9)\n\n    def cmd_callback(self, msg):\n        \"\"\"Store command data for system identification\"\"\"\n        cmd_data = [msg.linear.x, msg.angular.z]\n        self.commands.append(cmd_data)\n\n    def robot_dynamics_model(self, params, state, command):\n        \"\"\"\n        Simplified robot dynamics model\n        params: [mass, damping, friction]\n        state: [position, velocity]\n        command: [linear_vel_cmd, angular_vel_cmd]\n        \"\"\"\n        mass, damping, friction = params\n\n        # Simplified dynamics: acceleration = (command - damping*vel - friction*sign(vel)) / mass\n        acceleration = (command[0] - damping * state[1] - friction * np.sign(state[1])) / mass\n\n        return acceleration\n\n    def simulation_error(self, params):\n        \"\"\"Calculate error between simulation and real data\"\"\"\n        if len(self.joint_positions) < 10:  # Need sufficient data\n            return float('inf')\n\n        total_error = 0.0\n        for i in range(1, min(len(self.joint_positions), len(self.commands))):\n            # Get state and command\n            pos_prev = self.joint_positions[i-1][0] if len(self.joint_positions[i-1]) > 0 else 0\n            vel_prev = self.joint_velocities[i-1][0] if len(self.joint_velocities[i-1]) > 0 else 0\n            state_prev = [pos_prev, vel_prev]\n\n            cmd = self.commands[i]\n\n            # Simulate with current parameters\n            dt = self.timestamps[i] - self.timestamps[i-1]\n            acceleration = self.robot_dynamics_model(params, state_prev, cmd)\n            vel_sim = vel_prev + acceleration * dt\n            pos_sim = pos_prev + vel_sim * dt\n\n            # Calculate error\n            pos_real = self.joint_positions[i][0] if len(self.joint_positions[i]) > 0 else 0\n            vel_real = self.joint_velocities[i][0] if len(self.joint_velocities[i]) > 0 else 0\n\n            pos_error = abs(pos_sim - pos_real)\n            vel_error = abs(vel_sim - vel_real)\n\n            total_error += pos_error + vel_error\n\n        return total_error\n\n    def perform_identification(self):\n        \"\"\"Perform system identification to match simulation to reality\"\"\"\n        if len(self.joint_positions) < 10:\n            self.get_logger().warn('Insufficient data for system identification')\n            return\n\n        # Optimize parameters to minimize simulation error\n        result = minimize(\n            self.simulation_error,\n            self.current_params,\n            method='BFGS',\n            options={'disp': True}\n        )\n\n        if result.success:\n            self.current_params = result.x\n            self.get_logger().info(f'System identification completed. New parameters: {self.current_params}')\n\n            # Publish identified parameters\n            param_msg = Float32MultiArray()\n            param_msg.data = self.current_params.tolist()\n            self.param_pub.publish(param_msg)\n\n            # Save parameters to file\n            self.save_parameters()\n        else:\n            self.get_logger().error('System identification failed')\n\n    def save_parameters(self):\n        \"\"\"Save identified parameters to file\"\"\"\n        try:\n            with open('/tmp/robot_parameters.pkl', 'wb') as f:\n                pickle.dump({\n                    'parameters': self.current_params,\n                    'timestamp': self.get_clock().now().nanoseconds / 1e9\n                }, f)\n            self.get_logger().info('Parameters saved to /tmp/robot_parameters.pkl')\n        except Exception as e:\n            self.get_logger().error(f'Failed to save parameters: {e}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    sys_id_node = SystemIdentificationNode()\n\n    try:\n        rclpy.spin(sys_id_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        sys_id_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(e.p,{children:"Validation and evaluation techniques:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"# Compare simulation vs real robot performance\n# Run the same task in simulation and on real robot\n\n# Example evaluation metrics\n# 1. Success rate\n# 2. Task completion time\n# 3. Trajectory accuracy\n# 4. Energy efficiency\n\n# Use simulation with identified parameters\nros2 run my_robot_package system_identification_node\n\n# Apply identified parameters to simulation\nros2 param set /gazebo_robot_controller mass 1.2\nros2 param set /gazebo_robot_controller damping 0.15\nros2 param set /gazebo_robot_controller friction 0.08\n"})}),"\n",(0,a.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Conceptual Question"}),': Explain the "reality gap" and its impact on sim-to-real transfer. What are the main factors that contribute to this gap?']}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Practical Exercise"}),": Implement a domain randomization technique for a simple robot task in simulation. Train a model with randomized parameters and evaluate its performance."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Code Challenge"}),": Create a system identification node that calibrates simulation parameters based on real robot data to minimize the reality gap."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Critical Thinking"}),": What are the limitations of current sim-to-real transfer techniques? How might emerging technologies like digital twins and advanced simulation engines address these limitations?"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(e.p,{children:"This chapter covered the critical challenge of simulation-to-real-world transfer in Physical AI. We explored the reality gap, domain randomization techniques, system identification methods, and evaluation approaches. Successful sim-to-real transfer requires careful consideration of visual, physical, and temporal differences between simulation and reality. Techniques like domain randomization, system identification, and progressive domain transfer help bridge this gap, enabling the deployment of AI models trained in simulation to real robots."})]})}function c(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(m,{...n})}):m(n)}},8453(n,e,i){i.d(e,{R:()=>s,x:()=>o});var t=i(6540);const a={},r=t.createContext(a);function s(n){const e=t.useContext(r);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:s(n.components),t.createElement(r.Provider,{value:e},n.children)}}}]);