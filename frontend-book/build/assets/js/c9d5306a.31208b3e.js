"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[456],{5317(n,e,t){t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"module-4-vision-language-action/chapter-2","title":"Chapter 2: Action Generation and Execution","description":"Generating and executing robot actions from vision-language inputs","source":"@site/docs/module-4-vision-language-action/chapter-2.md","sourceDirName":"module-4-vision-language-action","slug":"/module-4-vision-language-action/chapter-2","permalink":"/HACKATHONN-1-Physical-AI-And-Humanoids-Robotics-Textbook/docs/module-4-vision-language-action/chapter-2","draft":false,"unlisted":false,"editUrl":"https://github.com/muhammadyasir678/HACKATHONN-1-Physical-AI-And-Humanoids-Robotics-Textbook/tree/main/docs/module-4-vision-language-action/chapter-2.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Chapter 2: Action Generation and Execution","sidebar_position":2,"description":"Generating and executing robot actions from vision-language inputs"},"sidebar":"textbookSidebar","previous":{"title":"Chapter 1: Vision-Language Models for Robotics","permalink":"/HACKATHONN-1-Physical-AI-And-Humanoids-Robotics-Textbook/docs/module-4-vision-language-action/chapter-1"},"next":{"title":"Chapter 3: Multimodal Learning Approaches","permalink":"/HACKATHONN-1-Physical-AI-And-Humanoids-Robotics-Textbook/docs/module-4-vision-language-action/chapter-3"}}');var i=t(4848),a=t(8453);const s={title:"Chapter 2: Action Generation and Execution",sidebar_position:2,description:"Generating and executing robot actions from vision-language inputs"},r="Chapter 2: Action Generation and Execution",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Core Theory",id:"core-theory",level:2},{value:"Practical Example",id:"practical-example",level:2},{value:"Code Snippet",id:"code-snippet",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Summary",id:"summary",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"chapter-2-action-generation-and-execution",children:"Chapter 2: Action Generation and Execution"})}),"\n",(0,i.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Understand the process of generating robot actions from vision-language inputs"}),"\n",(0,i.jsx)(e.li,{children:"Learn about action space representation and mapping"}),"\n",(0,i.jsx)(e.li,{children:"Explore techniques for action execution and control"}),"\n",(0,i.jsx)(e.li,{children:"Gain knowledge of safety considerations in action generation"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(e.p,{children:"Action generation and execution form the crucial bridge between high-level vision-language understanding and low-level robot control. In Vision-Language-Action (VLA) systems, the challenge is to translate natural language commands and visual observations into precise, executable robot actions. This process involves multiple stages: interpreting the visual scene, understanding the language command, mapping these inputs to an appropriate action space, and executing the action safely and effectively. The success of VLA systems depends heavily on the quality of this action generation and execution pipeline."}),"\n",(0,i.jsx)(e.h2,{id:"core-theory",children:"Core Theory"}),"\n",(0,i.jsx)(e.p,{children:"The action generation process involves several key components:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Action Space Representation"}),": How robot actions are encoded and structured"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Vision-Language Fusion"}),": Combining visual and linguistic information"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Action Mapping"}),": Converting high-level commands to low-level actions"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Execution Planning"}),": Sequencing actions for complex tasks"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Safety Constraints"}),": Ensuring safe and appropriate action execution"]}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:"Action spaces can be represented as:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Joint Space"}),": Direct control of robot joint angles"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Cartesian Space"}),": Control of end-effector position and orientation"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Task Space"}),": High-level task-specific parameters"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Discrete Actions"}),": Predefined action primitives"]}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:"The action generation pipeline typically includes:"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Perception"}),": Understanding the visual scene and identifying relevant objects"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Language Processing"}),": Parsing and interpreting the natural language command"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Fusion"}),": Combining visual and linguistic information"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Planning"}),": Determining the sequence of actions needed"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Control"}),": Converting high-level actions to low-level control commands"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Execution"}),": Safely executing the planned actions"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"practical-example",children:"Practical Example"}),"\n",(0,i.jsx)(e.p,{children:"Let's examine how to implement action generation from vision-language inputs:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport numpy as np\nfrom transformers import CLIPProcessor, CLIPModel\nimport cv2\n\nclass VisionLanguageActionGenerator(nn.Module):\n    def __init__(self, action_space_dim, hidden_dim=512):\n        super(VisionLanguageActionGenerator, self).__init__()\n\n        # Vision encoder (CLIP vision model)\n        self.vision_encoder = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").vision_model\n\n        # Text encoder (CLIP text model)\n        self.text_encoder = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").text_model\n\n        # Fusion layer to combine vision and language features\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(512 + 512, hidden_dim),  # Assuming 512-dim features from CLIP\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU()\n        )\n\n        # Action prediction head\n        self.action_head = nn.Sequential(\n            nn.Linear(hidden_dim, action_space_dim),\n            nn.Tanh()  # Normalize actions to [-1, 1]\n        )\n\n        # Action decoder to convert normalized actions to robot-specific commands\n        self.action_decoder = ActionDecoder(action_space_dim)\n\n    def forward(self, images, text_tokens):\n        # Encode visual features\n        vision_features = self.vision_encoder(pixel_values=images).pooler_output\n\n        # Encode text features\n        text_features = self.text_encoder(input_ids=text_tokens).pooler_output\n\n        # Combine vision and language features\n        combined_features = torch.cat([vision_features, text_features], dim=-1)\n        fused_features = self.fusion_layer(combined_features)\n\n        # Predict normalized actions\n        normalized_actions = self.action_head(fused_features)\n\n        # Decode to robot-specific action space\n        robot_actions = self.action_decoder(normalized_actions)\n\n        return robot_actions\n\nclass ActionDecoder:\n    def __init__(self, action_space_dim):\n        self.action_space_dim = action_space_dim\n        # Define action space boundaries for different robot types\n        self.action_bounds = {\n            '7dof_arm': {\n                'position': np.array([[-2.9, 2.9], [-1.8, 1.8], [-2.9, 2.9],  # Joint 1-3\n                                      [-3.1, 0.0], [-2.9, 2.9], [-3.8, 3.8],  # Joint 4-6\n                                      [-2.9, 2.9]]),  # Joint 7\n                'velocity': np.array([[-1.0, 1.0]] * 7),\n                'effort': np.array([[-87.0, 87.0]] * 7)\n            },\n            'mobile_base': {\n                'linear': [-1.0, 1.0],  # Linear velocity\n                'angular': [-1.0, 1.0]  # Angular velocity\n            }\n        }\n\n    def __call__(self, normalized_actions):\n        # Decode normalized actions to robot-specific range\n        robot_type = '7dof_arm'  # This would be determined dynamically\n        bounds = self.action_bounds[robot_type]\n\n        if robot_type == '7dof_arm':\n            # Decode joint positions\n            joint_positions = np.zeros(7)\n            for i in range(7):\n                min_val, max_val = bounds['position'][i]\n                joint_positions[i] = normalized_actions[i] * (max_val - min_val) / 2 + (max_val + min_val) / 2\n\n            return joint_positions\n        elif robot_type == 'mobile_base':\n            # Decode mobile base velocities\n            linear_vel = normalized_actions[0] * (bounds['linear'][1] - bounds['linear'][0]) / 2 + (bounds['linear'][1] + bounds['linear'][0]) / 2\n            angular_vel = normalized_actions[1] * (bounds['angular'][1] - bounds['angular'][0]) / 2 + (bounds['angular'][1] + bounds['angular'][0]) / 2\n            return np.array([linear_vel, angular_vel])\n\n        return normalized_actions\n\ndef execute_action(robot, action, duration=1.0):\n    \"\"\"\n    Execute the generated action on the robot\n    \"\"\"\n    # Convert action to robot-specific command\n    robot_cmd = convert_action_to_robot_command(action)\n\n    # Execute command with safety checks\n    if safety_check(robot, robot_cmd):\n        robot.execute_command(robot_cmd, duration)\n        return True\n    else:\n        print(\"Safety check failed, action not executed\")\n        return False\n\ndef safety_check(robot, command):\n    \"\"\"\n    Perform safety checks before executing command\n    \"\"\"\n    # Check joint limits\n    current_joints = robot.get_joint_positions()\n    new_joints = current_joints + command  # Simplified\n    joint_limits = robot.get_joint_limits()\n\n    for i, (pos, limit) in enumerate(zip(new_joints, joint_limits)):\n        if pos < limit[0] or pos > limit[1]:\n            return False\n\n    # Check for collisions (simplified)\n    if robot.would_collide(command):\n        return False\n\n    return True\n\ndef convert_action_to_robot_command(action):\n    \"\"\"\n    Convert normalized action to robot-specific command format\n    \"\"\"\n    # This would be robot-specific\n    return action\n"})}),"\n",(0,i.jsx)(e.h2,{id:"code-snippet",children:"Code Snippet"}),"\n",(0,i.jsx)(e.p,{children:"Example of action execution with safety monitoring:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import rospy\nimport numpy as np\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import JointState\nfrom std_msgs.msg import Float64MultiArray\nfrom control_msgs.msg import FollowJointTrajectoryAction, FollowJointTrajectoryGoal\nfrom trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\nimport actionlib\nimport threading\nimport time\n\nclass VLActionExecutor:\n    def __init__(self):\n        # Initialize ROS node\n        rospy.init_node(\'vla_action_executor\')\n\n        # Publishers for different robot types\n        self.velocity_pub = rospy.Publisher(\'/cmd_vel\', Twist, queue_size=10)\n        self.joint_cmd_pub = rospy.Publisher(\'/joint_group_position_controller/command\',\n                                           Float64MultiArray, queue_size=10)\n\n        # Subscribers for state feedback\n        self.joint_state_sub = rospy.Subscriber(\'/joint_states\', JointState, self.joint_state_callback)\n        self.laser_sub = rospy.Subscriber(\'/scan\', LaserScan, self.laser_callback)\n\n        # Action client for trajectory execution\n        self.trajectory_client = actionlib.SimpleActionClient(\n            \'/joint_trajectory_action\',\n            FollowJointTrajectoryAction\n        )\n\n        # Robot state\n        self.current_joint_positions = {}\n        self.current_joint_velocities = {}\n        self.obstacle_distances = []\n\n        # Safety parameters\n        self.safety_threshold = 0.3  # meters\n        self.max_velocity = 0.5\n        self.execution_lock = threading.Lock()\n\n        # Action execution parameters\n        self.default_duration = 2.0  # seconds\n\n        rospy.loginfo(\'VLA Action Executor initialized\')\n\n    def joint_state_callback(self, msg):\n        """Update current joint state"""\n        with self.execution_lock:\n            self.current_joint_positions = dict(zip(msg.name, msg.position))\n            self.current_joint_velocities = dict(zip(msg.name, msg.velocity))\n\n    def laser_callback(self, msg):\n        """Update obstacle distance information"""\n        with self.execution_lock:\n            self.obstacle_distances = [r for r in msg.ranges if 0 < r < float(\'inf\')]\n\n    def execute_vla_action(self, action_type, action_params, language_command):\n        """\n        Execute action based on VLA system output\n        """\n        rospy.loginfo(f"Executing action: {action_type} with params: {action_params}")\n\n        # Perform safety checks\n        if not self.safety_check():\n            rospy.logerr("Safety check failed, aborting action execution")\n            return False\n\n        # Execute appropriate action based on type\n        if action_type == "move_to_object":\n            return self.execute_move_to_object(action_params, language_command)\n        elif action_type == "grasp_object":\n            return self.execute_grasp_object(action_params, language_command)\n        elif action_type == "navigate_to":\n            return self.execute_navigation(action_params, language_command)\n        elif action_type == "manipulate_object":\n            return self.execute_manipulation(action_params, language_command)\n        else:\n            rospy.logerr(f"Unknown action type: {action_type}")\n            return False\n\n    def execute_move_to_object(self, params, language_command):\n        """Execute move-to-object action"""\n        # Extract target object information from language command\n        target_object = self.extract_object_from_command(language_command)\n\n        # Get target pose (would come from perception system)\n        target_pose = params.get(\'target_pose\', [0.5, 0.5, 0.0])  # x, y, z\n\n        # Plan path to object\n        path = self.plan_path_to_object(target_pose)\n\n        if path:\n            # Execute trajectory\n            success = self.execute_trajectory(path)\n            return success\n        else:\n            rospy.logerr(f"Could not plan path to {target_object}")\n            return False\n\n    def execute_grasp_object(self, params, language_command):\n        """Execute grasp object action"""\n        # Extract object information\n        target_object = self.extract_object_from_command(language_command)\n\n        # Get grasp pose\n        grasp_pose = params.get(\'grasp_pose\', [0.5, 0.5, 0.5, 0.0, 0.0, 0.0, 1.0])  # [x,y,z, qx,qy,qz,qw]\n\n        # Plan grasp trajectory\n        grasp_trajectory = self.plan_grasp_trajectory(grasp_pose)\n\n        if grasp_trajectory:\n            # Execute grasp\n            success = self.execute_trajectory(grasp_trajectory)\n\n            if success:\n                # Close gripper\n                self.close_gripper()\n                rospy.loginfo(f"Successfully grasped {target_object}")\n\n            return success\n        else:\n            rospy.logerr(f"Could not plan grasp for {target_object}")\n            return False\n\n    def execute_navigation(self, params, language_command):\n        """Execute navigation action"""\n        # Extract destination from language command\n        destination = self.extract_destination_from_command(language_command)\n\n        # Get navigation goal\n        goal = params.get(\'goal\', [1.0, 1.0, 0.0])  # x, y, theta\n\n        # Plan navigation path\n        path = self.plan_navigation_path(goal)\n\n        if path:\n            # Execute navigation\n            success = self.execute_navigation_path(path)\n            return success\n        else:\n            rospy.logerr(f"Could not plan navigation to {destination}")\n            return False\n\n    def execute_manipulation(self, params, language_command):\n        """Execute manipulation action"""\n        # Extract manipulation details from command\n        manipulation_type = params.get(\'manipulation_type\', \'pick_place\')\n\n        if manipulation_type == \'pick_place\':\n            # Execute pick and place sequence\n            pick_pose = params.get(\'pick_pose\')\n            place_pose = params.get(\'place_pose\')\n\n            success = self.execute_pick_place(pick_pose, place_pose)\n            return success\n        else:\n            rospy.logerr(f"Unknown manipulation type: {manipulation_type}")\n            return False\n\n    def extract_object_from_command(self, command):\n        """Extract object name from language command"""\n        # Simplified extraction - in real implementation, use NLP\n        command_lower = command.lower()\n\n        # Common object keywords\n        objects = [\'box\', \'cup\', \'bottle\', \'book\', \'ball\', \'toy\', \'object\']\n\n        for obj in objects:\n            if obj in command_lower:\n                return obj\n\n        return \'object\'  # Default\n\n    def extract_destination_from_command(self, command):\n        """Extract destination from language command"""\n        # Simplified extraction\n        command_lower = command.lower()\n\n        if \'kitchen\' in command_lower:\n            return \'kitchen\'\n        elif \'living room\' in command_lower or \'livingroom\' in command_lower:\n            return \'living room\'\n        elif \'bedroom\' in command_lower:\n            return \'bedroom\'\n        else:\n            return \'destination\'\n\n    def plan_path_to_object(self, target_pose):\n        """Plan path to target object"""\n        # Simplified path planning - in real implementation, use navigation stack\n        current_pose = self.get_current_pose()\n\n        # Create simple linear path\n        path = []\n        steps = 10\n        for i in range(steps + 1):\n            t = i / steps\n            x = current_pose[0] + t * (target_pose[0] - current_pose[0])\n            y = current_pose[1] + t * (target_pose[1] - current_pose[1])\n            path.append([x, y])\n\n        return path\n\n    def plan_grasp_trajectory(self, grasp_pose):\n        """Plan trajectory for grasping"""\n        # Simplified trajectory planning\n        current_pose = self.get_current_pose()\n\n        # Approach, grasp, lift trajectory\n        trajectory = [\n            [current_pose[0], current_pose[1], grasp_pose[2] + 0.1],  # Approach above\n            grasp_pose[:3],  # Grasp position\n            [grasp_pose[0], grasp_pose[1], grasp_pose[2] + 0.05]  # Lift slightly\n        ]\n\n        return trajectory\n\n    def execute_trajectory(self, trajectory):\n        """Execute planned trajectory"""\n        for waypoint in trajectory:\n            # Convert waypoint to joint positions (simplified)\n            joint_positions = self.cartesian_to_joint(waypoint)\n\n            if joint_positions is not None:\n                # Send joint command\n                self.send_joint_command(joint_positions)\n\n                # Wait for execution\n                time.sleep(0.5)\n            else:\n                rospy.logerr("Could not compute joint positions for waypoint")\n                return False\n\n        return True\n\n    def send_joint_command(self, joint_positions):\n        """Send joint position command to robot"""\n        cmd_msg = Float64MultiArray()\n        cmd_msg.data = joint_positions\n        self.joint_cmd_pub.publish(cmd_msg)\n\n    def close_gripper(self):\n        """Close robot gripper"""\n        # Simplified gripper control\n        gripper_cmd = Float64MultiArray()\n        gripper_cmd.data = [0.0]  # Close position\n        # Publish to gripper topic (implementation-specific)\n\n    def safety_check(self):\n        """Perform safety checks before execution"""\n        with self.execution_lock:\n            # Check for obstacles\n            if self.obstacle_distances:\n                min_distance = min(self.obstacle_distances)\n                if min_distance < self.safety_threshold:\n                    rospy.logwarn(f"Obstacle detected at {min_distance:.2f}m, below threshold {self.safety_threshold}m")\n                    return False\n\n            # Check joint limits\n            # Check velocity limits\n            # Check other safety constraints\n\n            return True\n\n    def get_current_pose(self):\n        """Get current robot pose"""\n        # Simplified - in real implementation, get from TF or odometry\n        return [0.0, 0.0, 0.0]\n\n    def cartesian_to_joint(self, cartesian_pose):\n        """Convert Cartesian pose to joint positions"""\n        # Simplified conversion - in real implementation, use inverse kinematics\n        # This is robot-specific and would use a kinematics solver\n        return [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  # Default joint positions\n\ndef main():\n    """Main function to run VLA action executor"""\n    executor = VLActionExecutor()\n\n    # Example usage - in real implementation, this would come from VLA system\n    action_type = "move_to_object"\n    action_params = {\n        \'target_pose\': [0.8, 0.6, 0.2]\n    }\n    language_command = "Move to the red box"\n\n    # Execute action\n    success = executor.execute_vla_action(action_type, action_params, language_command)\n\n    if success:\n        rospy.loginfo("Action executed successfully")\n    else:\n        rospy.logerr("Action execution failed")\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(e.p,{children:"Action execution monitoring and safety:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"# Monitor action execution\nrostopic echo /joint_states\nrostopic echo /cmd_vel\nrostopic echo /robot_status\n\n# Check robot safety status\nrostopic echo /safety_status\n\n# Emergency stop\nrostopic pub /emergency_stop std_msgs/Empty\n\n# Action execution feedback\nrostopic echo /action_execution_feedback\n"})}),"\n",(0,i.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Conceptual Question"}),": Explain the challenges involved in mapping high-level vision-language commands to low-level robot actions. What are the key components of this mapping process?"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Practical Exercise"}),": Implement a simple action executor that takes a language command and visual input, then generates appropriate robot motion commands."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Code Challenge"}),": Create a safety monitoring system that checks for potential collisions and joint limits before executing VLA-generated actions."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Critical Thinking"}),": How do safety considerations in VLA systems differ from traditional robotics approaches? What additional challenges arise when using vision-language inputs for action generation?"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(e.p,{children:"This chapter explored action generation and execution in Vision-Language-Action systems, which is crucial for bridging high-level understanding with low-level robot control. We covered action space representation, the action generation pipeline, and safety considerations. The success of VLA systems depends on the quality of the action generation and execution pipeline, which must translate natural language commands and visual observations into precise, safe robot actions. Understanding these concepts is essential for developing effective VLA systems that can operate reliably in real-world environments."})]})}function p(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453(n,e,t){t.d(e,{R:()=>s,x:()=>r});var o=t(6540);const i={},a=o.createContext(i);function s(n){const e=o.useContext(a);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:s(n.components),o.createElement(a.Provider,{value:e},n.children)}}}]);